{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import sys\n",
    "import json\n",
    "class Dictionary(object):\n",
    "    def __init__(self, word2idx=None, idx2word=None):\n",
    "        if word2idx is None:\n",
    "            word2idx = {}\n",
    "        if idx2word is None:\n",
    "            idx2word = []\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "    @property\n",
    "    def ntoken(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    @property\n",
    "    def padding_idx(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, sentence, add_word):\n",
    "        sentence = sentence.lower()\n",
    "        if \"? -yes/no\" in sentence:\n",
    "            sentence = sentence.replace(\"? -yes/no\", \"\")\n",
    "        if \"? -open\" in sentence:\n",
    "            sentence = sentence.replace(\"? -open\", \"\")\n",
    "        if \"? - open\" in sentence:\n",
    "            sentence = sentence.replace(\"? - open\", \"\")\n",
    "        sentence = sentence.replace(',', '').replace('?', '').replace('\\'s', ' \\'s').replace('...', '').replace('x ray', 'x-ray').replace('.', '')\n",
    "        words = sentence.split()\n",
    "        tokens = []\n",
    "        if add_word:\n",
    "            for w in words:\n",
    "                tokens.append(self.add_word(w))\n",
    "        else:\n",
    "            for w in words:\n",
    "                # if a word is not in dictionary, it will be replaced with the last word of dictionary.\n",
    "                tokens.append(self.word2idx.get(w, self.padding_idx-1))\n",
    "        return tokens\n",
    "\n",
    "    def dump_to_file(self, path):\n",
    "        cPickle.dump([self.word2idx, self.idx2word], open(path, 'wb'))\n",
    "        print('dictionary dumped to %s' % path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, path):\n",
    "        print('loading dictionary from %s' % path)\n",
    "        word2idx, idx2word = cPickle.load(open(path, 'rb'))\n",
    "        d = cls(word2idx, idx2word)\n",
    "        return d\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "def create_dictionary(dataroot):\n",
    "    dictionary = Dictionary()\n",
    "    questions = []\n",
    "    files = [\n",
    "        'trainset.json', #train dataset\n",
    "        'valset.json' #validate dateset\n",
    "    ]\n",
    "    for path in files:\n",
    "        qa_pairs = os.path.join(dataroot, path)\n",
    "        print(\"processing the {}\".format(path))\n",
    "        with open(qa_pairs) as f:\n",
    "            data_js = json.load(f)\n",
    "            for item in data_js:\n",
    "                dictionary.tokenize(item['question'], True)     #row[0]: id , row[1]: question , row[2]: answer\n",
    "    return dictionary\n",
    "\n",
    "def create_glove_embedding_init(idx2word, glove_file):\n",
    "    word2emb = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        entries = f.readlines()\n",
    "    emb_dim = len(entries[0].split(' ')) - 1\n",
    "    print('embedding dim is %d' % emb_dim)\n",
    "    weights = np.zeros((len(idx2word), emb_dim), dtype=np.float32)\n",
    "\n",
    "    for entry in entries:\n",
    "        vals = entry.split(' ')\n",
    "        word = vals[0]\n",
    "        vals = list(map(float, vals[1:]))\n",
    "        word2emb[word] = np.array(vals)\n",
    "    for idx, word in enumerate(idx2word):\n",
    "        if word not in word2emb:\n",
    "            continue\n",
    "        weights[idx] = word2emb[word]\n",
    "    return weights, word2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the trainset.json\n",
      "processing the valset.json\n",
      "dictionary dumped to /home/coder/projects/Med-VQA/data_OVQA/dictionary.pkl\n",
      "loading dictionary from /home/coder/projects/Med-VQA/data_OVQA/dictionary.pkl\n",
      "embedding dim is 300\n"
     ]
    }
   ],
   "source": [
    "# 替换了dictionary.pkl用这个生成/home/coder/projects/MEVF/MICCAI19-MedVQA/tools/create_dictionary.py\n",
    "\n",
    "\n",
    "data_dir = \"/home/coder/projects/Med-VQA/data_OVQA\"\n",
    "d = create_dictionary(data_dir)\n",
    "d.dump_to_file(data_dir + '/dictionary.pkl')\n",
    "\n",
    "d = Dictionary.load_from_file(data_dir + '/dictionary.pkl')\n",
    "emb_dim = 300\n",
    "glove_file = data_dir + '/glove/glove.6B.%dd.txt' % emb_dim\n",
    "weights, word2emb = create_glove_embedding_init(d.idx2word, glove_file)\n",
    "np.save(data_dir + '/glove6b_init_%dd.npy' % emb_dim, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import _pickle as cPickle\n",
    "\n",
    "contractions = {\n",
    "    \"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\":\n",
    "    \"could've\", \"couldnt\": \"couldn't\", \"couldn'tve\": \"couldn't've\",\n",
    "    \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\":\n",
    "    \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \"hadnt've\":\n",
    "    \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\":\n",
    "    \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \"he'dve\":\n",
    "    \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\",\n",
    "    \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \"Im\":\n",
    "    \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\":\n",
    "    \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n",
    "    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\":\n",
    "    \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n",
    "    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\",\n",
    "    \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n",
    "    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\":\n",
    "    \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\":\n",
    "    \"she'd've\", \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\":\n",
    "    \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\":\n",
    "    \"shouldn't've\", \"somebody'd\": \"somebodyd\", \"somebodyd've\":\n",
    "    \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\":\n",
    "    \"somebody'll\", \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\",\n",
    "    \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n",
    "    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\":\n",
    "    \"something'd\", \"somethingd've\": \"something'd've\", \"something'dve\":\n",
    "    \"something'd've\", \"somethingll\": \"something'll\", \"thats\":\n",
    "    \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n",
    "    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\":\n",
    "    \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \"they'dve\":\n",
    "    \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\":\n",
    "    \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \"wed've\":\n",
    "    \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\":\n",
    "    \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \"whats\":\n",
    "    \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\":\n",
    "    \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \"whod\":\n",
    "    \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\":\n",
    "    \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n",
    "    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\":\n",
    "    \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n",
    "    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\":\n",
    "    \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\",\n",
    "    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\":\n",
    "    \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \"youll\":\n",
    "    \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"\n",
    "}\n",
    "manual_map = { 'none': '0',\n",
    "              'zero': '0',\n",
    "              'one': '1',\n",
    "              'two': '2',\n",
    "              'three': '3',\n",
    "              'four': '4',\n",
    "              'five': '5',\n",
    "              'six': '6',\n",
    "              'seven': '7',\n",
    "              'eight': '8',\n",
    "               'nine': '9',\n",
    "              'ten': '10'}\n",
    "articles = ['a', 'an', 'the']\n",
    "period_strip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "comma_strip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "punct = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                '>', '<', '@', '`', ',', '?', '!']\n",
    "\n",
    "def process_punctuation(inText):\n",
    "    outText = inText\n",
    "    for p in punct:\n",
    "        if (p + ' ' in inText or ' ' + p in inText) \\\n",
    "           or (re.search(comma_strip, inText) != None):\n",
    "            outText = outText.replace(p, '')\n",
    "        else:\n",
    "            outText = outText.replace(p, ' ')\n",
    "    outText = period_strip.sub(\"\", outText, re.UNICODE)\n",
    "    return outText\n",
    "\n",
    "def process_digit_article(inText):\n",
    "    outText = []\n",
    "    tempText = inText.lower().split()\n",
    "    for word in tempText:\n",
    "        word = manual_map.setdefault(word, word)\n",
    "        if word not in articles:\n",
    "            outText.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    for wordId, word in enumerate(outText):\n",
    "        if word in contractions:\n",
    "            outText[wordId] = contractions[word]\n",
    "    outText = ' '.join(outText)\n",
    "    return outText\n",
    "\n",
    "def preprocess_answer(answer):\n",
    "    answer = str(answer)\n",
    "    answer = process_digit_article(process_punctuation(answer))\n",
    "    answer = answer.replace(',', '').replace('x ray', 'xray')\n",
    "    return answer\n",
    "\n",
    "def filter_answers(qa_pairs, min_occurence):\n",
    "    \"\"\"This will change the answer to preprocessed version\n",
    "    \"\"\"\n",
    "    occurence = {}\n",
    "\n",
    "    for id, row in qa_pairs.iterrows(): # row:[id,ques,ans]\n",
    "        gtruth = row['answer']\n",
    "        gtruth = ' '.join(gtruth.split())\n",
    "        # gtruth = preprocess_answer(gtruth)\n",
    "        if gtruth not in occurence:\n",
    "            occurence[gtruth] = set()\n",
    "        occurence[gtruth].add(row['question'])\n",
    "    for answer in list(occurence):\n",
    "        if len(occurence[answer]) < min_occurence:\n",
    "            occurence.pop(answer)\n",
    "\n",
    "    print('Num of answers that appear >= %d times: %d' % (\n",
    "        min_occurence, len(occurence)))\n",
    "    return occurence\n",
    "\n",
    "def create_ans2label(occurence,root='data'):\n",
    "    \"\"\"Note that this will also create label2ans.pkl at the same time\n",
    "\n",
    "    occurence: dict {answer -> whatever}\n",
    "    name: prefix of the output file\n",
    "    cache_root: str\n",
    "    \"\"\"\n",
    "    ans2label = {}\n",
    "    label2ans = []\n",
    "    label = 0\n",
    "    for answer in occurence:\n",
    "        label2ans.append(answer)\n",
    "        ans2label[answer] = label\n",
    "        label += 1\n",
    "\n",
    "    print('ans2lab', len(ans2label))\n",
    "    print('lab2abs', len(label2ans))\n",
    "\n",
    "    file = os.path.join(root, 'ans2label.pkl')\n",
    "    cPickle.dump(ans2label, open(file, 'wb'))\n",
    "    file = os.path.join(root, 'label2ans.pkl')\n",
    "    cPickle.dump(label2ans, open(file, 'wb'))\n",
    "    return ans2label\n",
    "\n",
    "def compute_target(answers_dset, ans2label, name, root='data'):\n",
    "    \"\"\"Augment answers_dset with soft score as label\n",
    "\n",
    "    ***answers_dset should be preprocessed***\n",
    "\n",
    "    Write result into a cache file\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    count = 0\n",
    "    for id,qa_pair in answers_dset.iterrows():\n",
    "        answers = ' '.join(qa_pair['answer'].split())\n",
    "        # answer_count = {}\n",
    "        # for answer in answers:\n",
    "        #     answer_ = answer['answer']\n",
    "        #     answer_count[answer_] = answer_count.get(answer_, 0) + 1\n",
    "\n",
    "        labels = []\n",
    "        scores = []\n",
    "        if answers in ans2label:\n",
    "            scores.append(1.)\n",
    "            labels.append(ans2label[answers])\n",
    "        # for answer in answer_count:\n",
    "        #     if answer not in ans2label:\n",
    "        #         continue\n",
    "        #     labels.append(ans2label[answer])\n",
    "        #     score = get_score(answer_count[answer])\n",
    "        #     scores.append(score)\n",
    "\n",
    "        target.append({\n",
    "            'question': qa_pair['question'],\n",
    "            'image_name': qa_pair['id'],\n",
    "            'labels': labels,\n",
    "            'scores': scores\n",
    "        })\n",
    "\n",
    "    file = os.path.join(root, name+'_target.pkl')\n",
    "    cPickle.dump(target, open(file, 'wb'))\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')\n",
    "train_path = os.path.join(data,'VQA-Med-2020-Task1-VQAnswering-TrainVal-Sets/VQAMed2020-VQAnswering-TrainingSet/VQAnswering_2020_Train_QA_pairs.txt')\n",
    "train_qa_pairs = pd.read_csv(train_path, sep='|', header=None, names=['id', 'question', 'answer'], index_col=None)\n",
    "occurence = filter_answers(train_qa_pairs, 0)  # select the answer with frequence over min_occurence\n",
    "\n",
    "label_path = data + 'ans2label.pkl'\n",
    "if os.path.isfile(label_path):\n",
    "    print('found %s' % label_path)\n",
    "    ans2label = cPickle.load(open(label_path, 'rb'))\n",
    "else:\n",
    "    ans2label = create_ans2label(occurence,data)     # create ans2label and label2ans\n",
    "\n",
    "compute_target(train_qa_pairs, ans2label, 'train',data) #dump train target to .pkl {question,image_name,labels,scores}\n",
    "\n",
    "validate_path = os.path.join(data,'VQA-Med-2020-Task1-VQAnswering-TrainVal-Sets/VQAMed2020-VQAnswering-ValidationSet/VQAnswering_2020_Val_QA_Pairs.txt')\n",
    "val_qa_pairs = pd.read_csv(validate_path, sep='|', header=None, names=['id', 'question', 'answer'], index_col=None)\n",
    "compute_target(val_qa_pairs, ans2label, 'validate', data)   #dump validate target to .pkl {question,image_name,labels,scores}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cache文件夹的文件使用/home/coder/projects/MEVF/MICCAI19-MedVQA/tools/create_label.py生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把qid转换为index_id ,并保存为json文件\n",
    "import json\n",
    "\n",
    "data_root = \"/home/coder/projects/Med-VQA/data_OVQA\"\n",
    "train_js = json.load(open(os.path.join(data_root, \"trainset.json\")))\n",
    "validation_js = json.load(open(os.path.join(data_root, \"valset.json\")))\n",
    "test_js = json.load(open(os.path.join(data_root, \"testset.json\")))\n",
    "\n",
    "# index = 0\n",
    "# pid2idx_js = {}\n",
    "# for type_js in [train_js, validation_js, test_js]:\n",
    "#     for js in type_js:\n",
    "#         ky = js['image_name']\n",
    "#         if ky in pid2idx_js:\n",
    "#             pass\n",
    "#         else:   \n",
    "#             pid2idx_js[ky] = index\n",
    "#             index += 1\n",
    "\n",
    "# # print(pid2idx_js)\n",
    "\n",
    "# json.dump(pid2idx_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/imgid2idx.json\", 'w'))\n",
    "\n",
    "\n",
    "index = 0\n",
    "pid2idx_js = {}\n",
    "for type_js in [train_js, validation_js, test_js]:\n",
    "    for js in type_js:\n",
    "        ky = js['image_name']\n",
    "        if ky in pid2idx_js:\n",
    "            pass\n",
    "        else:   \n",
    "            pid2idx_js[ky] = index\n",
    "            index += 1\n",
    "\n",
    "# print(pid2idx_js)\n",
    "\n",
    "json.dump(pid2idx_js, open(os.path.join(data_root, \"imgid2idx.json\"), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:57<00:00, 17.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def imageresize(img2idx_jsonpath, img_folderpath, reshape_size, out_path, channels):\n",
    "    with open(img2idx_jsonpath) as f:\n",
    "        img2idx = json.load(f)\n",
    "    \n",
    "    if channels == 3:\n",
    "        imgs = np.ndarray(shape=(len(img2idx), reshape_size, reshape_size, 3), dtype=float)\n",
    "    else:\n",
    "        imgs = np.ndarray(shape=(len(img2idx), reshape_size, reshape_size, 1), dtype=float)\n",
    "\n",
    "    for imgid, idx in tqdm(img2idx.items()):\n",
    "        if \".jpg\" in imgid or \".png\" in imgid:\n",
    "            imgpath = os.path.join(img_folderpath, imgid)\n",
    "        else:\n",
    "            imgpath = os.path.join(img_folderpath, f\"{imgid}.png\")\n",
    "        if os.path.exists(imgpath):\n",
    "            if channels == 3:\n",
    "                img = Image.open(imgpath).convert('RGB')\n",
    "            else:\n",
    "                img = Image.open(imgpath).convert('L')\n",
    "        else:\n",
    "            raise ValueError(f\"Image path is not correct: {imgpath}\")\n",
    "        resized = img.resize((reshape_size, reshape_size))\n",
    "        normalized = np.array(resized) / 255\n",
    "        if channels == 3:\n",
    "            normalized = normalized.reshape((reshape_size, reshape_size, 3))\n",
    "        else:\n",
    "            normalized = normalized.reshape((reshape_size, reshape_size, 1))\n",
    "        imgs[idx] = normalized\n",
    "\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(imgs, f)\n",
    "    return\n",
    "\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_OVQA/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_OVQA/img\", \n",
    "#             84, \"/home/coder/projects/Med-VQA/data_OVQA/images84x84.pkl\", 1)\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_OVQA/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_OVQA/img\", \n",
    "#             128, \"/home/coder/projects/Med-VQA/data_OVQA/images128x128.pkl\", 1)\n",
    "imageresize(\"/home/coder/projects/SystemDataset/data_OVQA_as_RAD/imgid2idx.json\", \"/home/coder/projects/SystemDataset/data_OVQA_as_RAD/images\", \n",
    "            224, \"/home/coder/projects/SystemDataset/data_OVQA_as_RAD/images224x224.pkl\", 3)\n",
    "\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_PATH/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_PATH/images\", \n",
    "#             84, \"/home/coder/projects/Med-VQA/data_PATH/images84x84.pkl\", 1)\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_PATH/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_PATH/images\", \n",
    "#             128, \"/home/coder/projects/Med-VQA/data_PATH/images128x128.pkl\", 1)\n",
    "\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_PATH/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_PATH/images\", \n",
    "#             224, \"/home/coder/projects/Med-VQA/data_PATH/images224x224.pkl\", 1)\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_PATH/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_PATH/images\", \n",
    "#             224, \"/home/coder/projects/Med-VQA/data_PATH/images224x224.pkl\", 3)\n",
    "\n",
    "# imageresize(\"/home/coder/projects/Med-VQA/data_SLAKE/imgid2idx.json\", \"/home/coder/projects/Med-VQA/data_SLAKE/images\", \n",
    "#             224, \"/home/coder/projects/Med-VQA/data_SLAKE/images224x224.pkl\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将ovqa处理为大模型的需要的数据格式在/home/coder/projects/MiniGPT-4/test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "target = torch.zeros(5)\n",
    "\n",
    "print(target)\n",
    "try:\n",
    "    target.scatter_(0, torch.tensor([3]), torch.tensor([1.]))\n",
    "except:\n",
    "    print('a_t=0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sys' has no attribute 'setdefaultencoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m  \n\u001b[0;32m----> 2\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetdefaultencoding\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sys' has no attribute 'setdefaultencoding'"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.setdefaultencoding('utf8') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('mmbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d482a1bf6d7fe2ffff62ba0108219233e6d040b3eba4b0dd8406aedad7490b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
