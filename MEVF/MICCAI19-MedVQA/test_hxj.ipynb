{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['which', 'organ', 'is', 'captured', 'by', 'this', 'ct', 'scan', 'what', 'system', 'primarily', 'present', 'in', 'image', 'imaged', 'the', 'mri', 'shows', 'shown', 'x-ray', 'one', 'seen', 'part', 'of', 'body', 'does', 'show', 'visualized', 'pictured', 'here', 'evaluated', 'systems', 'can', 'be', 'with', 'being', 'showing', 'principally', 'angiogram', 'displayed', 'ultrasound', 'gastrointestinal', 'pet', 'nuclear', 'medicine', 'abnormal', 'abnormality', 'primary', 'most', 'alarming', 'about', 'mammograph', 'look', 'normal', 'there', 'something', 'wrong', 'a', 'are', 'abnormalities', 'evidence', 'any', 'an', 'plane', 'taken', 'was', 'film', 'used', 'acquired', 'demonstrated', 'imaging', 'depicted', 'oriented', 'kind', 't1', 'weighted', 'type', 'modality', 'to', 'acquire', 'noncontrast', 'contrast', 'or', 'mr', 'weighting', 'represent', 'gi', 'given', 'patient', 'take', 'did', 'have', 'picture', 'how', 'iv', 'method', 't2', 'flair']\n",
      "dictionary dumped to /home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training/dictionary_test_hxj.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, word2idx=None, idx2word=None):\n",
    "        if word2idx is None:\n",
    "            word2idx = {}\n",
    "        if idx2word is None:\n",
    "            idx2word = []\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "    @property\n",
    "    def ntoken(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    @property\n",
    "    def padding_idx(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, sentence, add_word):\n",
    "        sentence = sentence.lower()\n",
    "        if \"? -yes/no\" in sentence:\n",
    "            sentence = sentence.replace(\"? -yes/no\", \"\")\n",
    "        if \"? -open\" in sentence:\n",
    "            sentence = sentence.replace(\"? -open\", \"\")\n",
    "        if \"? - open\" in sentence:\n",
    "            sentence = sentence.replace(\"? - open\", \"\")\n",
    "        sentence = sentence.replace(',', '').replace('?', '').replace('\\'s', ' \\'s').replace('...', '').replace('x ray', 'x-ray').replace('.', '')\n",
    "        words = sentence.split()\n",
    "        tokens = []\n",
    "        if add_word:\n",
    "            for w in words:\n",
    "                tokens.append(self.add_word(w))\n",
    "        else:\n",
    "            for w in words:\n",
    "                # if a word is not in dictionary, it will be replaced with the last word of dictionary.\n",
    "                tokens.append(self.word2idx.get(w, self.padding_idx-1))\n",
    "        return tokens\n",
    "\n",
    "    def dump_to_file(self, path):\n",
    "        cPickle.dump([self.word2idx, self.idx2word], open(path, 'wb'))\n",
    "        print('dictionary dumped to %s' % path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, path):\n",
    "        print('loading dictionary from %s' % path)\n",
    "        word2idx, idx2word = cPickle.load(open(path, 'rb'))\n",
    "        d = cls(word2idx, idx2word)\n",
    "        return d\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) + 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "def create_dictionary(dataroot):\n",
    "    dictionary = Dictionary()\n",
    "    questions = []\n",
    "    files = [\n",
    "        'traindf.csv',\n",
    "    ]\n",
    "    for path in files:\n",
    "        question_path = os.path.join(dataroot, path)\n",
    "        qs = pd.read_csv(open(question_path))\n",
    "        # print(len(qs))\n",
    "        for q in qs['question']:\n",
    "            # print(q)\n",
    "            dictionary.tokenize(q, True)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "# med2019_dir = \"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training\"\n",
    "ovqa_dir = \"/home/coder/projects/Med-VQA/data_OVQA\"\n",
    "data_dir = ovqa_dir\n",
    "dic = create_dictionary(data_dir)\n",
    "print(dic.idx2word)\n",
    "dic.dump_to_file(data_dir + '/dictionary_test_hxj.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##\n"
     ]
    }
   ],
   "source": [
    "print(\"##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def create_glove_embedding_init(idx2word, glove_file):\n",
    "    word2emb = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        entries = f.readlines()\n",
    "    emb_dim = len(entries[0].split(' ')) - 1\n",
    "    print('embedding dim is %d' % emb_dim)\n",
    "    weights = np.zeros((len(idx2word), emb_dim), dtype=np.float32)\n",
    "\n",
    "    for entry in entries:\n",
    "        vals = entry.split(' ')\n",
    "        word = vals[0]\n",
    "        vals = list(map(float, vals[1:]))\n",
    "        word2emb[word] = np.array(vals)\n",
    "    for idx, word in enumerate(idx2word):\n",
    "        if word not in word2emb:\n",
    "            continue\n",
    "        weights[idx] = word2emb[word]\n",
    "    return weights, word2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv(open(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training/traindf.csv\"))\n",
    "\n",
    "df.columns=df.columns.str.replace('img_id', 'image_name')\n",
    "df[\"image_name\"] = df[\"image_name\"] + \".jpg\"\n",
    "js = df.to_json(orient=\"records\",force_ascii=False)\n",
    "json.dump(json.loads(js), open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\", 'w'))\n",
    "\n",
    "# /home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Test/testdf.csv\n",
    "df = pd.read_csv(open(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Test/testdf.csv\"))\n",
    "\n",
    "df.columns=df.columns.str.replace('img_id', 'image_name')\n",
    "df[\"image_name\"] = df[\"image_name\"] + \".jpg\"\n",
    "js = df.to_json(orient=\"records\",force_ascii=False)\n",
    "json.dump(json.loads(js), open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/testset.json\", 'w', encoding='utf-8'))\n",
    "\n",
    "df = pd.read_csv(open(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Validation/valdf.csv\"))\n",
    "\n",
    "df.columns=df.columns.str.replace('img_id', 'image_name')\n",
    "df[\"image_name\"] = df[\"image_name\"] + \".jpg\"\n",
    "js = df.to_json(orient=\"records\",force_ascii=False)\n",
    "json.dump(json.loads(js), open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\", 'w', encoding='utf-8'))\n",
    "\n",
    "\n",
    "\n",
    "def create_jsons():\n",
    "    # read data\n",
    "    ## 添加参数 error_bad_lines=False by hxj\n",
    "    train_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\"))\n",
    "    validation_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\"))\n",
    "    test_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/testset.json\"))\n",
    "\n",
    "    # # convert df rows to dict\n",
    "    # logger.info(\"Converting each row in dataframe to dictionary...\")\n",
    "    # train_df.drop(columns=['id'], inplace=True)\n",
    "    # validation_df.drop(columns=['id'], inplace=True)\n",
    "    # test_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    ## add full image paths\n",
    "    train_image_dir = os.path.join(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training/train_images\")\n",
    "    validation_image_dir = os.path.join(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Validation/val_images\")\n",
    "    test_image_dir = os.path.join(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Test/test_images\")\n",
    "\n",
    "    for js in train_js:\n",
    "        path = _imgpath(train_image_dir, js['image_name'])\n",
    "        if path != \"nofile\":\n",
    "            js['qid'] = path\n",
    "        else: \n",
    "            del js\n",
    "    for js in validation_js:\n",
    "        path = _imgpath(validation_image_dir, js['image_name'])\n",
    "        if path != \"nofile\":\n",
    "            js['qid'] = path\n",
    "        else: \n",
    "            del js\n",
    "    for js in test_js:\n",
    "        path = _imgpath(test_image_dir, js['image_name'])\n",
    "        if path != \"nofile\":\n",
    "            js['qid'] = path\n",
    "        else: \n",
    "            del js\n",
    "\n",
    "    json.dump(train_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\", 'w'))\n",
    "    json.dump(validation_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\", 'w'))\n",
    "    json.dump(test_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/testset.json\", 'w'))\n",
    "\n",
    "create_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'image_name': 'synpic41148.jpg', 'question': 'which organ is captured by this ct scan?', 'answer': 'lung, mediastinum, pleura', 'mode': 'train', 'category': 'organ'}\n",
      "{'image_name': 'synpic41148.jpg', 'question': 'which organ is captured by this ct scan?', 'answer': 'lung, mediastinum, pleura', 'mode': 'train', 'category': 'organ', 'id': '/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training/train_images/synpic41148.jpg'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _imgpath(img_dir, name):\n",
    "    # print(name)\n",
    "    img_path = os.path.join(img_dir, str(name))\n",
    "    if not os.path.exists(img_path):\n",
    "        return \"nofile\"\n",
    "    return img_path\n",
    "\n",
    "js_test = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\"))\n",
    "print(type(js_test))\n",
    "\n",
    "for js in js_test:\n",
    "    print(js)\n",
    "    break\n",
    "\n",
    "train_image_dir = \"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training/train_images\"\n",
    "# js_test = list(map(lambda x:  imgpath(train_image_dir, x['image_name'])), js_test)\n",
    "for js in js_test:\n",
    "    js['id'] = _imgpath(train_image_dir, js['image_name'])\n",
    "# list(map(lambda x: print(x['image_name']), js_test))\n",
    "\n",
    "# print(js_test)\n",
    "for js in js_test:\n",
    "    print(js)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 打开.pkl文件，将数据加载到一个Python对象中\n",
    "with open('/home/coder/projects/MEVF/MICCAI19-MedVQA/data_RAD/images224x224.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# 使用数据\n",
    "print(data.shape)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_jsons():\n",
    "    # read data\n",
    "    ## 添加参数 error_bad_lines=False by hxj\n",
    "    train_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\"))\n",
    "    validation_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\"))\n",
    "    test_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\"))\n",
    "\n",
    "    # # convert df rows to dict\n",
    "    # logger.info(\"Converting each row in dataframe to dictionary...\")\n",
    "    # train_df.drop(columns=['id'], inplace=True)\n",
    "    # validation_df.drop(columns=['id'], inplace=True)\n",
    "    # test_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    ## add full image paths\n",
    "    train_image_dir = os.path.join(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Training/train_images\")\n",
    "    validation_image_dir = os.path.join(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Validation/val_images\")\n",
    "    test_image_dir = os.path.join(\"/home/coder/projects/MMBERT/VQA-Med-2019/ImageClef-2019-VQA-Med-Test/test_images\")\n",
    "\n",
    "    for js in train_js:\n",
    "        path = _imgpath(train_image_dir, js['image_name'])\n",
    "        if path != \"nofile\":\n",
    "            js['id'] = path\n",
    "        else: \n",
    "            del js\n",
    "    for js in validation_js:\n",
    "        path = _imgpath(validation_image_dir, js['image_name'])\n",
    "        if path != \"nofile\":\n",
    "            js['id'] = path\n",
    "        else: \n",
    "            del js\n",
    "    for js in test_js:\n",
    "        path = _imgpath(test_image_dir, js['image_name'])\n",
    "        if path != \"nofile\":\n",
    "            js['id'] = path\n",
    "        else: \n",
    "            del js\n",
    "\n",
    "    json.dump(train_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\", 'w'))\n",
    "    json.dump(validation_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\", 'w'))\n",
    "    json.dump(test_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/testset.json\", 'w'))\n",
    "\n",
    "create_jsons()\n",
    "\n",
    "    ### drop files that don't exist: for some names in csv files, the actual image does not exist\n",
    "    # train_df = train_df[train_df['name'] != \"nofile\"]\n",
    "    # validation_df = validation_df[validation_df['name'] != \"nofile\"]\n",
    "    # test_df = test_df[test_df['name'] != \"nofile\"]\n",
    "\n",
    "    ### drop zero bytes images\n",
    "    # train_df['imagesize'] = train_df['name'].apply(lambda x: _imgsize(x))\n",
    "    # validation_df['imagesize'] = validation_df['name'].apply(lambda x: _imgsize(x))\n",
    "    # test_df['imagesize'] = test_df['name'].apply(lambda x: _imgsize(x))\n",
    "\n",
    "    # train_df = train_df[train_df['imagesize'] != 0]\n",
    "    # validation_df = validation_df[validation_df['imagesize'] != 0]\n",
    "    # test_df = test_df[test_df['imagesize'] != 0]\n",
    "\n",
    "    # train_df.drop(columns=['imagesize'], inplace=True)\n",
    "    # validation_df.drop(columns=['imagesize'], inplace=True)\n",
    "    # test_df.drop(columns=['imagesize'], inplace=True)\n",
    "\n",
    "    # train_df.rename(columns={\"name\": \"image_path\"}, inplace=True)\n",
    "    # validation_df.rename(columns={\"name\": \"image_path\"}, inplace=True)\n",
    "    # test_df.rename(columns={\"name\": \"image_path\"}, inplace=True)\n",
    "\n",
    "    # ## convert to dict\n",
    "    # train_dict = train_df.to_dict('index')\n",
    "    # validation_dict = validation_df.to_dict('index')\n",
    "    # test_dict = test_df.to_dict('index')\n",
    "\n",
    "    # del [[train_df, validation_df, test_df]]\n",
    "    # gc.collect()\n",
    "\n",
    "    # # Dump to json\n",
    "    # ## train\n",
    "    # logger.info(\"Dumping json data for train dataset...\")\n",
    "    # with open(os.path.join(jsonpath, \"train_dataset.json\"), 'w') as f:\n",
    "    #     for row in tqdm(train_dict):\n",
    "    #         json.dump(train_dict[row], f)\n",
    "    #         f.write(\"\\n\")\n",
    "    # ## validation\n",
    "    # logger.info(\"Dumping json data for validation dataset...\")\n",
    "    # with open(os.path.join(jsonpath, \"validation_dataset.json\"), 'w') as f:\n",
    "    #     for row in tqdm(validation_dict):\n",
    "    #         json.dump(validation_dict[row], f)\n",
    "    #         f.write(\"\\n\")\n",
    "    # ## test\n",
    "    # logger.info(\"Dumping json data for test dataset...\")\n",
    "    # with open(os.path.join(jsonpath, \"test_dataset.json\"), 'w') as f:\n",
    "    #     for row in tqdm(test_dict):\n",
    "    #         json.dump(test_dict[row], f)\n",
    "    #         f.write(\"\\n\")\n",
    "\n",
    "logger.info(\"Jsons are successfly created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#把qid转换为index_id ,并保存为json文件\n",
    "import json\n",
    "\n",
    "train_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/trainset.json\"))\n",
    "validation_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/valset.json\"))\n",
    "test_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/testset.json\"))\n",
    "\n",
    "# index = 0\n",
    "# pid2idx_js = {}\n",
    "# for type_js in [train_js, validation_js, test_js]:\n",
    "#     for js in type_js:\n",
    "#         ky = js['image_name']\n",
    "#         if ky in pid2idx_js:\n",
    "#             pass\n",
    "#         else:   \n",
    "#             pid2idx_js[ky] = index\n",
    "#             index += 1\n",
    "\n",
    "# # print(pid2idx_js)\n",
    "\n",
    "# json.dump(pid2idx_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/imgid2idx.json\", 'w'))\n",
    "\n",
    "\n",
    "index = 0\n",
    "pid2idx_js = {}\n",
    "for type_js in [train_js, validation_js, test_js]:\n",
    "    for js in type_js:\n",
    "        ky = js['qid']\n",
    "        if ky in pid2idx_js:\n",
    "            pass\n",
    "        else:   \n",
    "            pid2idx_js[ky] = index\n",
    "            index += 1\n",
    "\n",
    "# print(pid2idx_js)\n",
    "\n",
    "json.dump(pid2idx_js, open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/imgid2idx_path_test.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#把图片转换为数组并保存为images.pkl\n",
    "#224x224x3!!!!!!!!!!!!!\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "tfm = transforms.Compose([transforms.ToPILImage(),  \n",
    "                            transforms.Resize([224, 224]),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "all_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/imgid2idx_path.json\"))\n",
    "\n",
    "image_np = np.empty((len(all_js), 224, 224, 3))\n",
    "for path, id in all_js.items():\n",
    "    # img = cv2.imread(path)\n",
    "    # img = tfm(img)\n",
    "\n",
    "    # 读取图片并转换为灰度图像\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    transform = transforms.Resize((224, 224))\n",
    "    img = transform(img)\n",
    "    # 将灰度图像转换为数组\n",
    "    img_array = np.asarray(img) / 255\n",
    "    img_array = np.reshape(img_array, (224, 224, 3))\n",
    "\n",
    "    image_np[id] = img_array\n",
    "\n",
    "print(image_np.shape)\n",
    "with open('/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/images224x224.pkl', 'wb') as f:\n",
    "    pickle.dump(image_np, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "#把图片转换为数组并保存为images.pkl\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "tfm = transforms.Compose([transforms.ToPILImage(),  \n",
    "                            transforms.Resize([84, 84]),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "all_js = json.load(open(\"/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/imgid2idx_path.json\"))\n",
    "\n",
    "image_np = np.empty((len(all_js), 84, 84, 1))\n",
    "for path, id in all_js.items():\n",
    "    # img = cv2.imread(path)\n",
    "    # img = tfm(img)\n",
    "\n",
    "    # 读取图片并转换为灰度图像\n",
    "    img = Image.open(path).convert('L')\n",
    "    transform = transforms.Resize((84, 84))\n",
    "    img = transform(img)\n",
    "    # 将灰度图像转换为数组\n",
    "    img_array = np.asarray(img) / 255\n",
    "    img_array = np.reshape(img_array, (84, 84, 1))\n",
    "\n",
    "    image_np[id] = img_array\n",
    "\n",
    "print(image_np.shape)\n",
    "with open('/home/coder/projects/MEVF/MICCAI19-MedVQA/data_Med/VQA-Med-2019/images84x84.pkl', 'wb') as f:\n",
    "    pickle.dump(image_np, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "# 从文件中加载对象\n",
    "with open('/home/coder/projects/Med-VQA/data_SLAKE/dictionary.pkl', 'rb') as f:\n",
    "    my_obj = pickle.load(f)\n",
    "    print(my_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CLOSED', 'OPEN'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "st = set()\n",
    "with open('/home/coder/projects/SystemDataset/data_OVQA_as_RAD/trainset.json', 'rb') as f:\n",
    "    train_js = json.load(f)\n",
    "    for js in train_js:\n",
    "        st.add(js[\"question_type\"])\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "labels = torch.tensor([3], dtype=torch.int64 )\n",
    "scores = torch.tensor([1], dtype=torch.int64 )\n",
    "target = torch.zeros(5, dtype=torch.int64 )\n",
    "if labels is not None:\n",
    "    # print(\"#####\", target.shape, labels, scores)\n",
    "    target.scatter_(0, labels, scores)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(answer['labels'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5cd6345f2f4575e712f4af2b639b80306d698b0fb4af248ce4f8a4e061c833b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
